# -*- coding: utf-8 -*-
"""rocauc

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/160LszXTbdMZRXk5R_j8mLVGWRck37ORt
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.utils.data
import torch.nn.functional as F
import torchvision
import torchvision.models as models
import matplotlib.pyplot as plt
import numpy as np
import gc
import os
import random
from torchvision import transforms
from PIL import Image, ImageFile
from math import trunc, log

from sklearn.metrics import recall_score
from sklearn.metrics import accuracy_score

from sklearn.metrics import roc_curve
from sklearn.metrics import auc

import matplotlib.pyplot as plt
import numpy as np
import itertools
import sklearn

!python --version
!nvcc --version

print("torch version: {}".format(torch.__version__))
print("torch vision version: {}".format(torchvision.__version__))
print("numpy version: {}".format(np.version.version))
print("sklearn version: {}".format(sklearn.__version__))

from google.colab import drive
drive.mount('/content/drive')

# seeds defined for REPRODUCIBILITY
# https://discuss.pytorch.org/t/reproducibility-with-all-the-bells-and-whistles/81097
# https://pytorch.org/docs/stable/notes/randomness.html

SEED = 2020

def seed_all(seed):
    if not seed:
        seed = 10

    print("[ Using Seed : ", seed, " ]")

    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.cuda.manual_seed(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def seed_worker(worker_id):
    worker_seed = torch.initial_seed() % 2**32
    np.random.seed(worker_seed)
    random.seed(worker_seed)

seed_all(SEED)

# extrair o dataset no colab
import zipfile
zip_ref = zipfile.ZipFile('/content/drive/My Drive/tg/datasets/2100-original-sanitized-binary-dataset.zip', 'r')
zip_ref.extractall("/input")
zip_ref.close()

from pathlib import Path

model_name = 'vgg16_bn'
folder_path = Path('/input/2100-original-sanitized-binay-dataset')
save_file = Path('/results/model-result/{}'.format(model_name))
results_path = Path('/results')

os.makedirs(save_file, exist_ok=True)

# constantes
im_size = 224
epochs = 2
epochs_ft = 30
batch_size = 32
testing_with = 'validation'
ImageFile.LOAD_TRUNCATED_IMAGES = True

# transformacoes usadas para o processo de treino
train_transform = transforms.Compose([
    transforms.RandomHorizontalFlip(p=0.5), # data augmentation
    transforms.RandomVerticalFlip(p=0.5),   # data augmentation
    transforms.Resize((im_size,im_size)), 
    transforms.ToTensor(),
    transforms.Normalize(
      mean=[0.485, 0.456, 0.406],
      std=[0.229, 0.224, 0.225])
    ])

# transformacoes ??
img_transforms = transforms.Compose([
    transforms.Resize((im_size,im_size)), 
    transforms.ToTensor(),
    transforms.Normalize(
      mean=[0.485, 0.456, 0.406],
      std=[0.229, 0.224, 0.225])
    ])

def check_image(path):
    try:
        im = Image.open(path)
        return True
    except:
        return False

gc.collect()

# particionar o conjunto de treino
train_data_path = folder_path/'train'
train_data = torchvision.datasets.ImageFolder(root=train_data_path, transform=train_transform, is_valid_file=check_image)
train_data_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,shuffle=True, worker_init_fn=seed_worker)

# particionar o conjunto de validacao
validation_data_path = folder_path/'valid'
validation_data = torchvision.datasets.ImageFolder(root=validation_data_path, transform=img_transforms, is_valid_file=check_image)
validation_data_loader = torch.utils.data.DataLoader(validation_data, batch_size=batch_size,shuffle=True, worker_init_fn=seed_worker)

print('Treino -> ' + str(train_data.class_to_idx))
print('Validacao -> ' + str(validation_data.class_to_idx))

transfer_model = models.vgg16_bn(pretrained=True)

# Freezing the convolutional layers, except batch normalizations.
for name, param in transfer_model.named_parameters():
    if("bn" not in name):
        param.requires_grad = True

# Other models supplied with PyTorch use either 'fc' or 'classifier'. You can
# ...also use 'out_features' to discover the activations coming out.

# transfer_model.classifier[0]

transfer_model.fc = nn.Sequential(
    nn.Linear(transfer_model.classifier[0].in_features, 500),
    nn.Softmax(dim=1),
    nn.Dropout(), 
    nn.Linear(500,2)
)

torch.save(transfer_model, results_path/f'{model_name}_architecture')

if torch.cuda.is_available():
    device = torch.device("cuda")
    print('Using CUDA...')
else:
    device = torch.device("cpu")

def find_lr(model, loss_fn, optimizer, train_loader, init_value=1e-8, final_value=10.0, device="cpu"):
    number_in_epoch = len(train_loader) - 1
    update_step = (final_value / init_value) ** (1 / number_in_epoch)
    lr = init_value
    optimizer.param_groups[0]["lr"] = lr
    best_loss = 0.0
    batch_num = 0
    losses = []
    log_lrs = []

    # jogar o model na gpu
    if torch.cuda.is_available():
      model.cuda()

    for data in train_loader:
        batch_num += 1
        
        # Progress
        if batch_num%10==0:
            print('Finding lr... nof data: ' + str(batch_num) + '+')
            
        inputs, targets = data
        inputs = inputs.to(device)
        targets = targets.to(device)
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = loss_fn(outputs, targets)

        # Crash out if loss explodes
        if batch_num > 1 and loss > 4 * best_loss:
            return log_lrs, losses

        # Record the best loss
        if loss < best_loss or batch_num == 1:
            best_loss = loss

        # Store the values
        losses.append(loss.item())
        log_lrs.append((lr))

        # Do the backward pass and optimize
        loss.backward()
        optimizer.step()

        # Update the lr for the next step and store
        lr *= update_step
        if lr > 1e-1:
            break
        optimizer.param_groups[0]["lr"] = lr
    return log_lrs, losses

optimizer = optim.Adam(transfer_model.parameters())

device

logs,losses = find_lr(transfer_model, torch.nn.CrossEntropyLoss(), optimizer, train_data_loader, device=device)

plt.plot(logs, losses)  # Plotting learning rate graph.
plt.xscale("log")
plt.xlabel("Learning rate")
plt.ylabel("Loss")
plt.show()
plt.savefig(save_file/'chart_lr_x_loss.png')

def distance(x):  # Logarithmic distance.
    offset = log(10)
    return -(log(x) - offset) / (log(1e-1) - offset)  # Distances grow towards value 0.

best = 0
found_lr = None
if len(logs) > 20: # if len(logs > 20) -> tava certo isso mesmo?
    logs = logs[10:-5] # Ignores the beginning and the end of the graph.
    losses = losses[10:-5]

for i in range(len(logs)):
    if i != 0:
        cur = (losses[i-1] - losses[i]) / (distance(logs[i]) - distance(logs[i-1]))
        if cur > best:
            best = cur
            found_lr = logs[i-1]
print('Reportedly optimal learning rate: ' + str(found_lr))

# funcao de treino da rede
train_loss_print = []
valid_loss_print = []

def train(model, optimizer, loss_fn, train_loader, val_loader, epochs=epochs, device="cpu"):
    for epoch in range(epochs):
        training_loss = 0.0
        valid_loss = 0.0
        model.train()

        # actuals_treino = []
        # actuals_validacao = []
        # predictions_treino = []
        # predictions_validacao = []

        for batch in train_loader:
            optimizer.zero_grad()
            inputs, targets = batch
            inputs = inputs.to(device)
            targets = targets.to(device)
            output = model(inputs)
            loss = loss_fn(output, targets)
            loss.backward()
            optimizer.step()
            training_loss += loss.data.item() * inputs.size(0)
            # fix
            # prediction = torch.max(F.softmax(output), dim=1)[1]
            # actuals_treino.extend(targets.view_as(prediction).cpu().numpy())
            # predictions_treino.extend(prediction.cpu().numpy())

        training_loss /= len(train_loader.dataset)
        train_loss_print.append(training_loss)

        model.eval()
        for batch in val_loader:
            inputs, targets = batch
            inputs = inputs.to(device)
            output = model(inputs)
            targets = targets.to(device)
            loss = loss_fn(output,targets) 
            valid_loss += loss.data.item() * inputs.size(0)

            # fix
            # prediction = torch.max(F.softmax(output), dim=1)[1]
            # actuals_validacao.extend(targets.view_as(prediction).cpu().numpy())
            # predictions_validacao.extend(prediction.cpu().numpy())

        valid_loss /= len(val_loader.dataset)
        valid_loss_print.append(valid_loss)

        print('Epoch: {}'.format(epoch))
        print('\tTraining Loss: {:.2f}'.format(training_loss))
        print('\tValidation Loss: {:.2f}'.format(valid_loss))
        # print('\tAccuracy(Treino): {:.2f}'.format(accuracy_score(actuals_treino, predictions_treino)))
        # print('\tAccuracy(Validacao): {:.2f}'.format(accuracy_score(actuals_validacao, predictions_validacao)))
        # print('\tRecall+(Treino): {:.2f}'.format(recall_score(actuals_treino, predictions_treino, pos_label=0)))
        # print('\tRecall+(Validacao): {:.2f}'.format(recall_score(actuals_validacao, predictions_validacao, pos_label=0)))
        # print('\tRecall-(Treino): {:.2f}'.format(recall_score(actuals_treino, predictions_treino, pos_label=1)))
        # print('\tRecall-(Validacao): {:.2f}'.format(recall_score(actuals_validacao, predictions_validacao, pos_label=1)))

gc.collect()

# Reload
transfer_model = torch.load(results_path/f'{model_name}_architecture')
os.remove(results_path/f'{model_name}_architecture')
transfer_model.to(device)

if found_lr == None:
    raise ValueError('Error finding learning rate!')

optimizer = optim.Adam(transfer_model.parameters(), lr=found_lr)

# Training
train(transfer_model, optimizer,torch.nn.CrossEntropyLoss(), train_data_loader, validation_data_loader, epochs=1, device=device)

# Save

torch.save(transfer_model.state_dict(), '/results/models/' + model_name + '_dict_' + str(epochs) + 'epochs')

model = transfer_model
num_correct = 0 
num_examples = 0
valid_loss = 0.0
loss_fn = torch.nn.CrossEntropyLoss()

if testing_with == 'validation':
    data_loader = validation_data_loader
elif testing_with == 'test':
    data_loader = test_data_loader
else:
    raise ValueError('Test dataset not defined or invalid!')

def computar_metricas(model, device, test_loader):
    model.eval()
    actuals = []
    predictions = []
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            prediction = output.argmax(dim=1, keepdim=True)
            predictions.extend(prediction)
            actuals.extend(target.view_as(prediction))
    return [i.item() for i in actuals], [i.item() for i in predictions]

loader = torch.utils.data.DataLoader(
    validation_data,
    batch_size=batch_size,
    shuffle=True)

# maybe acc vs epochs

# exp 2
# resnet
# vgg
# - com e sem DA ()

actuals, predictions = computar_metricas(model, device, loader)

len(actuals)

from sklearn.metrics import confusion_matrix

print('Accuracy(loader): {:.2f}'.format(accuracy_score(actuals, predictions)))
print('Recall+(loader): {:.2f}'.format(recall_score(actuals, predictions, pos_label=0)))
print('Recall-(loader): {:.2f}'.format(recall_score(actuals, predictions, pos_label=1)))
print('Confusion matrix:')
cm = confusion_matrix(actuals, predictions)
print(cm)

start = torch.cuda.Event(enable_timing=True)
end = torch.cuda.Event(enable_timing=True)

def test_class_probabilities(model, device, test_loader, which_class):
    start.record()
    model.eval()
    actuals = []
    probabilities = []
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            prediction = output.argmax(dim=1, keepdim=True)
            actuals.extend(target.view_as(prediction) == which_class)
            probabilities.extend(np.exp(torch.Tensor.cpu(output[:, which_class])))
    end.record()
    return [i.item() for i in actuals], [i.item() for i in probabilities]

# model.eval()
    # actuals = []
    # probabilities = []
    # predictions = []
    # actuals_sem_ser_bool = []
    # with torch.no_grad():
    #     for data, target in test_loader:
    #         data, target = data.to(device), target.to(device)
    #         output = model(data)
    #         prediction = output.argmax(dim=1, keepdim=True)
    #         predictions.extend(prediction)
    #         actuals.extend(target.view_as(prediction) == which_class)
    #         probabilities.extend(np.exp(torch.Tensor.cpu(output[:, which_class])))
    #         actuals_sem_ser_bool.extend(target.view_as(prediction))
    # return [i.item() for i in actuals], [i.item() for i in probabilities], [i.item() for i in predictions], [i.item() for i in actuals_sem_ser_bool]

test_loader = torch.utils.data.DataLoader(
    validation_data,
    batch_size=batch_size,
    shuffle=True)

torch.cuda.synchronize()

positive_class = 0
actuals, class_probabilities = test_class_probabilities(model, device, test_loader, positive_class)

print(start.elapsed_time(end))

fpr, tpr, _ = roc_curve(actuals, class_probabilities)

roc_auc = auc(fpr, tpr)

plt.figure()
lw = 2
plt.plot(fpr, tpr, color='darkorange',
         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC for digit=%d class' % positive_class)
plt.legend(loc="lower right")
plt.show()

# plotar a matriz de confusao formatada
def plot_confusion_matrix(cm, target_names, title='Matriz de ConfusÃ£o', cmap=None, normalize=True):

    accuracy = np.trace(cm) / float(np.sum(cm))
    misclass = 1 - accuracy

    if cmap is None:
        cmap = plt.get_cmap('Greys')

    plt.figure(figsize=(5, 5))
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title, fontsize = 20)
    plt.colorbar()

    if target_names is not None:
        tick_marks = np.arange(len(target_names))
        plt.xticks(tick_marks, target_names, fontsize = 16)
        plt.yticks(tick_marks, target_names, fontsize = 16)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

    thresh = cm.max() / 1.5 if normalize else cm.max() / 2
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        if normalize:
            plt.text(j, i, "{:0.4f}".format(cm[i, j]),
                     horizontalalignment="center",
                     fontsize=16,
                     color="white" if cm[i, j] > thresh else "black")
        else:
            plt.text(j, i, "{:,}".format(cm[i, j]),
                     horizontalalignment="center",
                     fontsize=16,
                     color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('Atual', fontsize = 20)
    plt.xlabel('Predito'.format(accuracy, misclass), fontsize = 20)
    plt.show()

plot_confusion_matrix(
    cm = np.array(cm),
    normalize = False,
    target_names = ['Bom', 'Ruim'],
    title = "")

# total = 0
# for row in confusion_matrix:
#     total += sum(row)

# target_names = []; longer = 0
# for folder in os.listdir(folder_path/'train'):
#     target_names.append(folder.capitalize())
#     longer = max(longer, len(folder))

# acc_avg = 0
# sens_avg = 0
# specf_avg = 0

# print('METRICS:\n')
# for i in range(len(confusion_matrix)):
#     print(('{:<' + str(longer) + '}').format(target_names[i]) + '   ')
#     TP = 0; TN = 0; FP = 0; FN = 0
#     for j in range(len(confusion_matrix[i])):  # Within the images of the class.
#         if i == j:
#             TP += confusion_matrix[i][j]  # Correctly identified.
#         else:
#             FP += confusion_matrix[i][j]  # All the others.
#     for j in range(len(confusion_matrix)):  # Within all identified as the class.
#         if j != i:
#             FN += confusion_matrix[j][i]  # Column of the class.
#     TN = total - (TP + FN + FP)
#     acc = float(TN + TP) / (TN + TP + FN + FP)
#     sens = float(TP) / (TP + FN)
#     specf = float(TN) / (TN + FP)
#     acc_avg += acc
#     sens_avg += sens
#     specf_avg += specf
#     print('Accuracy - {:>12.4f}'.format(acc) + ' ')
#     print('Sensitivity - {:>12.4f}'.format(sens) + ' ')
#     print('Specificity - {:>12.4f}'.format(specf) + '\n')

# For balanced datasets
# acc_avg /= num_classes
# sens_avg /= num_classes
# specf_avg /= num_classes

# print('\n\n=========================\n\nAccuracy avg:    ' + '{:.4f}'.format(acc_avg) + '\nSensitivity avg: ' +'{:.4f}'.format(sens_avg) + '\nSpecificity avg: ' + '{:.4f}'.format(specf_avg) + '\n')
# print('\nlr: ' + str(found_lr) + '\n')

# printar grafico loss treino e validacao
epochs_arr = range(0, 1)

fig, ax1 = plt.subplots()
color = 'tab:red'
ax1.set_xlabel('Epochs')
ax1.set_ylabel('Loss', color=color)
ax1.plot(epochs_arr, train_loss_print, color=color)
ax1.tick_params(axis='y', labelcolor=color)

ax2 = ax1.twinx()

color = 'tab:blue'
ax2.plot(epochs_arr, valid_loss_print, color=color)
ax2.tick_params(axis='y', labelcolor=color)

fig.tight_layout()  # otherwise the right y-label is slightly clipped
plt.show()


# plt.plot(epochs_arr, train_loss_print, valid_loss_print)  # Plotting learning rate graph.




# plt.xlabel("Epochs")
# plt.ylabel("Loss")
# plt.show()
# plt.savefig(save_file/'epochsVsLoss.png')

arr = [1,2,3,4,5,6]
brr = [11.123 ,12.1212, 13.1212, 14.111, 15.111, 16.111]

type(arr) == type(preds)

np.savetxt("aa1.txt", Arr)

with open("aa1.txt", "ab") as f:
    np.savetxt(f, Brr)

content = np.loadtxt("aa1.txt") 
print("\nContent in file2.txt:\n", content)

"""Meta colab"""

#GPU count and name
!nvidia-smi -L

#use this command to see GPU activity while doing Deep Learning tasks, for this command 'nvidia-smi' and for above one to work, go to 'Runtime > change runtime type > Hardware Accelerator > GPU'
!nvidia-smi

!lscpu |grep 'Model name'

#no.of sockets i.e available slots for physical processors
!lscpu | grep 'Socket(s):'

#no.of cores each processor is having 
!lscpu | grep 'Core(s) per socket:'

#no.of threads each core is having
!lscpu | grep 'Thread(s) per core'

!lscpu | grep "L3 cache"

#if it had turbo boost it would've shown Min and Max MHz also but it is only showing current frequency this means it always operates at shown frequency
!lscpu | grep "MHz"

#memory that we can use
!free -h --si | awk  '/Mem:/{print $2}'

#hard disk space that we can use
!df -h / | awk '{print $4}'